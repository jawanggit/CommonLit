{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T15:37:25.248928Z",
     "iopub.status.busy": "2021-05-26T15:37:25.248542Z",
     "iopub.status.idle": "2021-05-26T15:37:27.864322Z",
     "shell.execute_reply": "2021-05-26T15:37:27.86316Z",
     "shell.execute_reply.started": "2021-05-26T15:37:25.248847Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from string import punctuation\n",
    "import re\n",
    "import lightgbm\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T15:37:27.866169Z",
     "iopub.status.busy": "2021-05-26T15:37:27.865923Z",
     "iopub.status.idle": "2021-05-26T15:37:27.973054Z",
     "shell.execute_reply": "2021-05-26T15:37:27.971966Z",
     "shell.execute_reply.started": "2021-05-26T15:37:27.866145Z"
    }
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\n",
    "train_data = pd.read_csv('../input/commonlitreadabilityprize/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T15:37:27.976353Z",
     "iopub.status.busy": "2021-05-26T15:37:27.976044Z",
     "iopub.status.idle": "2021-05-26T15:37:27.980735Z",
     "shell.execute_reply": "2021-05-26T15:37:27.97991Z",
     "shell.execute_reply.started": "2021-05-26T15:37:27.976321Z"
    }
   },
   "outputs": [],
   "source": [
    "train = train_data.copy()\n",
    "test = test_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T15:37:27.982306Z",
     "iopub.status.busy": "2021-05-26T15:37:27.982093Z",
     "iopub.status.idle": "2021-05-26T15:37:28.203033Z",
     "shell.execute_reply": "2021-05-26T15:37:28.201862Z",
     "shell.execute_reply.started": "2021-05-26T15:37:27.982283Z"
    }
   },
   "outputs": [],
   "source": [
    "print(train.target.mean())\n",
    "train.target.plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T15:37:28.206288Z",
     "iopub.status.busy": "2021-05-26T15:37:28.206004Z",
     "iopub.status.idle": "2021-05-26T15:37:28.229466Z",
     "shell.execute_reply": "2021-05-26T15:37:28.228586Z",
     "shell.execute_reply.started": "2021-05-26T15:37:28.206259Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_char(text):\n",
    "    num = random.randint(15,35)\n",
    "    num2 = random.randint(0,1)\n",
    "    \n",
    "    if num2 == 0:\n",
    "        return text+text[0:num]\n",
    "    else:\n",
    "        return text[:-num]\n",
    "# ------------------------------------\n",
    "def target_jitter(df):\n",
    "    lst = list(zip(df.target, df.standard_error))\n",
    "    final_lst = []\n",
    "    for val in lst:\n",
    "        num = random.randint(0,1)\n",
    "        error = random.uniform(0, val[1])\n",
    "        if num ==0:\n",
    "            final_lst.append(val[0]+error)\n",
    "        else:\n",
    "            final_lst.append(val[0]-error)\n",
    "    df['target'] = final_lst\n",
    "    return df\n",
    "    \n",
    "    \n",
    "# ------------------------------------\n",
    "def word_length_counts(text):\n",
    "    '''\n",
    "        Return: \n",
    "            Dictionry of words length counts \n",
    "    '''\n",
    "    \n",
    "    d = {\"1_letter\": 0, \n",
    "         \"2_letter\": 0, \n",
    "         \"3_letter\": 0, \n",
    "         \"4_letter\": 0, \n",
    "         \"5_letter\": 0, \n",
    "         \"6_letter\": 0, \n",
    "         \"7_letter\": 0, \n",
    "         \"8_letter\": 0, \n",
    "         \"9_letter\": 0, \n",
    "         \"10_letter\": 0, \n",
    "         \"11_letter\": 0, \n",
    "         \"12_letter\": 0\n",
    "    }\n",
    "\n",
    "    text.replace('\\n', '')\n",
    "    text = text.lower()\n",
    "    n_text = ''\n",
    "    for c in text: \n",
    "        if c not in punctuation:\n",
    "            n_text += c\n",
    "        else: n_text += ' '\n",
    "\n",
    "    text = n_text\n",
    "    for word in text.split(' '): \n",
    "        wl = len(word)\n",
    "        if wl > 0:\n",
    "            if len(word) >= 12: key = f\"12_letter\"\n",
    "            else:  key = f\"{len(word)}_letter\"\n",
    "\n",
    "            d[key] += 1; \n",
    "    return d\n",
    "# ------------------------------------\n",
    "def source_info(text):\n",
    "    '''\n",
    "        Return: \n",
    "            Dictionary of source wiki, article, book, details, story or stories, kid, edu, simple\n",
    "    '''\n",
    "    d = dict()\n",
    "    source_type = ['wiki', 'article', 'book', 'details', 'kid', 'edu', 'simple', 'story', 'stories']\n",
    "    \n",
    "    for t in source_type:\n",
    "        if t != 'stories': \n",
    "            d[t] = 0\n",
    "            \n",
    "        if t in text:\n",
    "            if t == 'stories': \n",
    "                d['story'] = 1;\n",
    "            else: d[t] = 1\n",
    "    return d\n",
    "# ------------------------------------\n",
    "def document_info(text):\n",
    "    '''\n",
    "        Return:\n",
    "            Dictionary of document lenght, word count, sentence count, average word lenght\n",
    "    '''\n",
    "    text_lenght = len(text)\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text_word_count = len(text.split(' '))\n",
    "    text_sentence_count = len(re.split('\\.|!|\\?',text)) # modified by James\n",
    "    text = re.split('\\.|!|\\?',text)\n",
    "    text_avg_word_length = round(sum([len(t) for t in text]) / text_word_count, 2)\n",
    "\n",
    "    document_info = {\n",
    "        'doc_len': text_lenght,\n",
    "        'word_count': text_word_count,\n",
    "        'sent_count': text_sentence_count,\n",
    "        'avg_word_len': text_avg_word_length\n",
    "    }\n",
    "\n",
    "    return document_info\n",
    "#----------------------------------------\n",
    "def words_per_sentence(text):\n",
    "    total = []\n",
    "    text = text.replace('Mrs.',\"Mrs\")\n",
    "    text = text.replace('Mr.',\"Mr\")\n",
    "    text = text.replace('Dr.',\"Dr\")\n",
    "    sentences = re.split('\\.|!|\\?',text)\n",
    "    for sentence in sentences: #iterate over list of sentences\n",
    "        if sentence != '':\n",
    "            word_list = sentence.split(' ') #split a sentence into list of words\n",
    "            while(\"\" in word_list):\n",
    "                word_list.remove(\"\")\n",
    "            while('\"' in word_list):\n",
    "                word_list.remove('\"')\n",
    "            total.append(len(word_list)) #total number of words in a sentence add to list\n",
    "    return np.mean(total)\n",
    "    \n",
    "# ------------------------------------\n",
    "def character_counts(text):\n",
    "    '''\n",
    "        Return:\n",
    "            Dictionary of counts of all characters in text\n",
    "    '''\n",
    "    char_dict = dict()\n",
    "    text = text.lower()\n",
    "    text = text.replace('\\n', ' ')\n",
    "    \n",
    "    for char in text:\n",
    "        if char not in char_dict:\n",
    "            char_dict[char] = 0\n",
    "        \n",
    "        char_dict[char] += 1     \n",
    "        \n",
    "    return char_dict\n",
    "\n",
    "# ------------------------------------\n",
    "def phonemes_counts(text):\n",
    "    '''\n",
    "        Return: \n",
    "            Dictionay of all phonemic in text\n",
    "    '''\n",
    "    phonemes = ['ck', 'cc', 'di', 'nn', 'dd', 'ai', 'ss', 'mn', 'bb', \n",
    "                'sci', 'ze', 'qu', 'se', 'sc', 'ci', 'ps', 'si', 'tch', \n",
    "                'ngue', 'st', 'gu', 'th', 'pn', 've', 'te', 'zz', 'au', \n",
    "                'lm', 'lf', 'ge', 'wh', 'tu', 'wr', 'ph', 'sh', 'mm', 'gh', \n",
    "                'dge', 'ft', 'tt', 'ed', 'ng', 'lk', 'ti', 'gue', 'rr', 'ch', \n",
    "                'll', 'gn', 'ff', 'gg', 'pp', 'rh', 'ce', 'mb', 'kn', \n",
    "                'eer', 'ere', 'uy', 'ho', 'ear', 'ei', 'ar', 'ai', \n",
    "                'oor', 'ure', 'eigh', 'ey', 'is', 'ae', 'ow', 'or', 'ew', \n",
    "                'ore', 'ur', 'uoy', 'air', 'au', 'ough', 'yr', \n",
    "                'ea', 'ayer', 'augh', 'aw', 'eau', 'aigh', 'igh', 'oy', \n",
    "                'oo', 'ue', 'are', 'ee', 'oa', 'et', 'y', 'er', 'eir', \n",
    "                'oew', 'oar', 'ie', 'eo', 'ui', 'ier', 'ou', 'ir', 'oi', \n",
    "                'ay', 'ye', 'oe', 'our']\n",
    "    temp_dict = dict()\n",
    "    \n",
    "    # lower text \n",
    "    text = text.lower()\n",
    "    \n",
    "    for p in phonemes:\n",
    "        temp_dict[p] = text.count(p)\n",
    "    \n",
    "    return temp_dict\n",
    "\n",
    "#-----------------------\n",
    "def add_data(df,float1,float2):\n",
    "    group = df.loc[(df.target<float1) | (df.target>float2)]\n",
    "    group = group.copy()\n",
    "    group['mod'] =  group.excerpt.apply(lambda x: remove_char(x))\n",
    "    group.drop(columns =['excerpt'],axis = 1,inplace = True)\n",
    "    group = group.rename(columns = {\"mod\":\"excerpt\"})\n",
    "    target_jitter(group)\n",
    "    mod_train = pd.concat([df,group],sort = 'False')\n",
    "    train = mod_train\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T15:37:28.230433Z",
     "iopub.status.busy": "2021-05-26T15:37:28.230245Z",
     "iopub.status.idle": "2021-05-26T15:37:28.487811Z",
     "shell.execute_reply": "2021-05-26T15:37:28.48627Z",
     "shell.execute_reply.started": "2021-05-26T15:37:28.230411Z"
    }
   },
   "outputs": [],
   "source": [
    "train = add_data(train,-2,1)\n",
    "train = add_data(train,-5,1)\n",
    "train = add_data(train,-5,1)\n",
    "train = add_data(train,-5,.5)\n",
    "train = add_data(train,-5,.5)\n",
    "train = add_data(train,-5,.5)\n",
    "train = add_data(train,-2,6)\n",
    "train = add_data(train,-2,6)\n",
    "\n",
    "\n",
    "train.target.plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T15:37:28.489756Z",
     "iopub.status.busy": "2021-05-26T15:37:28.489414Z",
     "iopub.status.idle": "2021-05-26T15:37:28.49736Z",
     "shell.execute_reply": "2021-05-26T15:37:28.496072Z",
     "shell.execute_reply.started": "2021-05-26T15:37:28.48973Z"
    }
   },
   "outputs": [],
   "source": [
    "print(train.target.mean())\n",
    "print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T15:37:28.500497Z",
     "iopub.status.busy": "2021-05-26T15:37:28.500137Z",
     "iopub.status.idle": "2021-05-26T15:37:28.526115Z",
     "shell.execute_reply": "2021-05-26T15:37:28.523816Z",
     "shell.execute_reply.started": "2021-05-26T15:37:28.500468Z"
    }
   },
   "outputs": [],
   "source": [
    "#combine train and test set for pre-processing\n",
    "c_df = pd.concat([train, test], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T15:37:28.528147Z",
     "iopub.status.busy": "2021-05-26T15:37:28.527845Z",
     "iopub.status.idle": "2021-05-26T15:37:28.56568Z",
     "shell.execute_reply": "2021-05-26T15:37:28.564722Z",
     "shell.execute_reply.started": "2021-05-26T15:37:28.528122Z"
    }
   },
   "outputs": [],
   "source": [
    "c_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T15:37:28.567048Z",
     "iopub.status.busy": "2021-05-26T15:37:28.566804Z",
     "iopub.status.idle": "2021-05-26T15:37:38.077835Z",
     "shell.execute_reply": "2021-05-26T15:37:38.076402Z",
     "shell.execute_reply.started": "2021-05-26T15:37:28.567021Z"
    }
   },
   "outputs": [],
   "source": [
    "series_documents =  c_df.excerpt.apply(lambda x: document_info(x))\n",
    "series_phonemes = c_df.excerpt.apply(lambda x: phonemes_counts(x))\n",
    "series_characters =  c_df.excerpt.apply(lambda x: character_counts(x))\n",
    "series_word_length = c_df.excerpt.apply(lambda x: word_length_counts(x))\n",
    "series_word_per_sent = c_df.excerpt.apply(lambda x: words_per_sentence(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T15:37:38.07957Z",
     "iopub.status.busy": "2021-05-26T15:37:38.079283Z",
     "iopub.status.idle": "2021-05-26T15:37:39.118819Z",
     "shell.execute_reply": "2021-05-26T15:37:39.116904Z",
     "shell.execute_reply.started": "2021-05-26T15:37:38.079541Z"
    }
   },
   "outputs": [],
   "source": [
    "df_documents = pd.DataFrame(list(series_documents))\n",
    "df_phonemes = pd.DataFrame(list(series_phonemes))\n",
    "df_chracters = pd.DataFrame(list(series_characters))\n",
    "df_word_length = pd.DataFrame(list(series_word_length))\n",
    "df_word_per_sent = pd.DataFrame(list(series_word_per_sent))\n",
    "df_word_per_sent.rename(columns={0:'wps'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T15:37:39.120898Z",
     "iopub.status.busy": "2021-05-26T15:37:39.120558Z",
     "iopub.status.idle": "2021-05-26T15:37:39.180499Z",
     "shell.execute_reply": "2021-05-26T15:37:39.178981Z",
     "shell.execute_reply.started": "2021-05-26T15:37:39.120863Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make na 0\n",
    "df_chracters.fillna(0, inplace = True)\n",
    "#df_chracters_test.fillna(0, inplace = True)\n",
    "\n",
    "\n",
    "# Make all ints (change NaN to 0)\n",
    "for col in df_chracters.columns:\n",
    "    df_chracters[col] = df_chracters[col].astype(int, copy=False)\n",
    "\n",
    "#for col in df_chracters_test.columns:\n",
    "#    df_chracters_test[col] = df_chracters_test[col].astype(int, copy=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T15:37:39.183761Z",
     "iopub.status.busy": "2021-05-26T15:37:39.183391Z",
     "iopub.status.idle": "2021-05-26T15:37:39.213183Z",
     "shell.execute_reply": "2021-05-26T15:37:39.212364Z",
     "shell.execute_reply.started": "2021-05-26T15:37:39.183729Z"
    }
   },
   "outputs": [],
   "source": [
    "df_dialogue = pd.DataFrame(list(c_df.excerpt.str.contains(\"\\\".*\\\"\",regex = True).astype(int)))\n",
    "df_dialogue.rename(columns = {0:'dialogue'},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T15:37:39.214585Z",
     "iopub.status.busy": "2021-05-26T15:37:39.214341Z",
     "iopub.status.idle": "2021-05-26T15:37:39.224021Z",
     "shell.execute_reply": "2021-05-26T15:37:39.22258Z",
     "shell.execute_reply.started": "2021-05-26T15:37:39.214558Z"
    }
   },
   "outputs": [],
   "source": [
    "df_X = pd.concat([df_documents, df_phonemes, df_word_length,df_word_per_sent,df_dialogue], axis = 1)\n",
    "#df_X_test = pd.concat([df_documents_test, df_phonemes_test, df_word_lenght_test], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T15:37:39.225851Z",
     "iopub.status.busy": "2021-05-26T15:37:39.225513Z",
     "iopub.status.idle": "2021-05-26T15:37:39.26703Z",
     "shell.execute_reply": "2021-05-26T15:37:39.265605Z",
     "shell.execute_reply.started": "2021-05-26T15:37:39.225822Z"
    }
   },
   "outputs": [],
   "source": [
    "df_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T15:37:39.268853Z",
     "iopub.status.busy": "2021-05-26T15:37:39.268565Z",
     "iopub.status.idle": "2021-05-26T15:37:39.273593Z",
     "shell.execute_reply": "2021-05-26T15:37:39.272457Z",
     "shell.execute_reply.started": "2021-05-26T15:37:39.268829Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_X.reset_index(drop=True,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T15:37:39.275242Z",
     "iopub.status.busy": "2021-05-26T15:37:39.274956Z",
     "iopub.status.idle": "2021-05-26T15:37:41.044971Z",
     "shell.execute_reply": "2021-05-26T15:37:41.043084Z",
     "shell.execute_reply.started": "2021-05-26T15:37:39.275217Z"
    }
   },
   "outputs": [],
   "source": [
    "df_X = df_X.T.drop_duplicates().T\n",
    "# df_X_test = df_X_test.T.drop_duplicates().T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T15:37:41.046827Z",
     "iopub.status.busy": "2021-05-26T15:37:41.046526Z",
     "iopub.status.idle": "2021-05-26T15:37:41.052818Z",
     "shell.execute_reply": "2021-05-26T15:37:41.051292Z",
     "shell.execute_reply.started": "2021-05-26T15:37:41.046801Z"
    }
   },
   "outputs": [],
   "source": [
    "df_X_test = df_X.iloc[-len(test):]\n",
    "df_X = df_X.iloc[:-len(test)]\n",
    "df_y = c_df['target'][:-len(test)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Standardize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T15:37:41.054602Z",
     "iopub.status.busy": "2021-05-26T15:37:41.054307Z",
     "iopub.status.idle": "2021-05-26T15:37:41.087992Z",
     "shell.execute_reply": "2021-05-26T15:37:41.087089Z",
     "shell.execute_reply.started": "2021-05-26T15:37:41.054577Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "print(scaler.fit(df_X))\n",
    "# print(scaler.data_max_)\n",
    "x_train = scaler.transform(df_X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Fitting Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T15:42:12.108262Z",
     "iopub.status.busy": "2021-05-26T15:42:12.107967Z",
     "iopub.status.idle": "2021-05-26T15:42:23.777205Z",
     "shell.execute_reply": "2021-05-26T15:42:23.776565Z",
     "shell.execute_reply.started": "2021-05-26T15:42:12.108237Z"
    }
   },
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#LightGBM\n",
    "model = LGBMRegressor(n_estimators = 600, max_depth = 60, max_features = 'sqrt')\n",
    "\n",
    "# # Number of trees in random forest\n",
    "# n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# # Number of features to consider at every split\n",
    "# max_features = ['auto', 'sqrt']\n",
    "# # Maximum number of levels in tree\n",
    "# max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "# max_depth.append(None)\n",
    "# # Minimum number of samples required to split a node\n",
    "# min_samples_split = [2, 5, 10]\n",
    "# # Minimum number of samples required at each leaf node\n",
    "# min_samples_leaf = [1, 2, 4]\n",
    "# # Method of selecting samples for training each tree\n",
    "# bootstrap = [True, False]\n",
    "# # Create the random grid\n",
    "# random_grid = {'n_estimators': n_estimators,\n",
    "#                'max_features': max_features,\n",
    "#                'max_depth': max_depth,\n",
    "#                'min_samples_split': min_samples_split,\n",
    "#                'min_samples_leaf': min_samples_leaf,\n",
    "#                'bootstrap': bootstrap}\n",
    "\n",
    "# lgbm = RandomizedSearchCV(estimator = model, param_distributions = random_grid, n_iter = 100, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n",
    "# lgbm.fit(x_train, df_y.values)\n",
    "\n",
    "# lgbm.best_params_\n",
    "\n",
    "model.fit(x_train,df_y)\n",
    "predictions = model.predict(x_train)\n",
    "print(mean_squared_error(df_y, predictions, squared=False))\n",
    "cv_results = cross_validate(model, x_train, df_y.values, cv=5, scoring = 'neg_mean_squared_error')\n",
    "\n",
    "\n",
    "# #Random Forest\n",
    "# model_rfr = RandomForestRegressor(n_estimators = 500, )\n",
    "# model_rfr.fit(x_train,df_y)\n",
    "# predictions = model_rfr.predict(x_train)\n",
    "# print(mean_squared_error(df_y, predictions, squared=False))\n",
    "# cv_results_rfr = cross_validate(model_rfr, x_train, df_y.values, cv=5, scoring = 'neg_mean_squared_error')\n",
    "# print(cv_results_rfr['test_score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T15:42:46.566107Z",
     "iopub.status.busy": "2021-05-26T15:42:46.565805Z",
     "iopub.status.idle": "2021-05-26T15:42:46.571219Z",
     "shell.execute_reply": "2021-05-26T15:42:46.570736Z",
     "shell.execute_reply.started": "2021-05-26T15:42:46.566084Z"
    }
   },
   "outputs": [],
   "source": [
    "cv_results['test_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "default LGBM was .33489"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.20189421949665898\n",
    "\n",
    "[-0.67757485 -0.29058851 -0.21928215 -0.22328962 -0.17938408]\n",
    "\n",
    "0.19931558921802858\n",
    "\n",
    "[-0.64393856 -0.27008002 -0.22100026 -0.22946819 -0.23954556]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
    "[LightGBM] [Warning] Unknown parameter: max_features\n",
    "[LightGBM] [Warning] Unknown parameter: min_samples_leaf\n",
    "[LightGBM] [Warning] Unknown parameter: bootstrap\n",
    "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
    "{'n_estimators': 600,\n",
    " 'min_samples_split': 5,\n",
    " 'min_samples_leaf': 1,\n",
    " 'max_features': 'sqrt',\n",
    " 'max_depth': 60,\n",
    " 'bootstrap': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T07:14:47.663053Z",
     "iopub.status.busy": "2021-05-26T07:14:47.662563Z",
     "iopub.status.idle": "2021-05-26T07:14:47.675162Z",
     "shell.execute_reply": "2021-05-26T07:14:47.674384Z",
     "shell.execute_reply.started": "2021-05-26T07:14:47.663022Z"
    }
   },
   "outputs": [],
   "source": [
    "#LightGBM\n",
    "x_test = scaler.transform(df_X_test)\n",
    "y_predict = model.predict(x_test)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T07:12:48.859159Z",
     "iopub.status.busy": "2021-05-26T07:12:48.85881Z",
     "iopub.status.idle": "2021-05-26T07:12:48.865062Z",
     "shell.execute_reply": "2021-05-26T07:12:48.864275Z",
     "shell.execute_reply.started": "2021-05-26T07:12:48.859128Z"
    }
   },
   "outputs": [],
   "source": [
    "#RFR\n",
    "x_test = scaler.transform(df_X_test)\n",
    "y_predict_rfr = model_rfr.predict(x_test)\n",
    "y_predict_rfr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T07:12:48.869613Z",
     "iopub.status.busy": "2021-05-26T07:12:48.869278Z",
     "iopub.status.idle": "2021-05-26T07:12:48.874542Z",
     "shell.execute_reply": "2021-05-26T07:12:48.873705Z",
     "shell.execute_reply.started": "2021-05-26T07:12:48.86958Z"
    }
   },
   "outputs": [],
   "source": [
    "# avg_result = (y_predict_rfr + y_predict)/2\n",
    "# avg_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T07:14:52.924391Z",
     "iopub.status.busy": "2021-05-26T07:14:52.923906Z",
     "iopub.status.idle": "2021-05-26T07:14:52.928405Z",
     "shell.execute_reply": "2021-05-26T07:14:52.92743Z",
     "shell.execute_reply.started": "2021-05-26T07:14:52.924357Z"
    }
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(y_predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T07:14:53.385293Z",
     "iopub.status.busy": "2021-05-26T07:14:53.384804Z",
     "iopub.status.idle": "2021-05-26T07:14:53.393796Z",
     "shell.execute_reply": "2021-05-26T07:14:53.393019Z",
     "shell.execute_reply.started": "2021-05-26T07:14:53.38526Z"
    }
   },
   "outputs": [],
   "source": [
    "submission['id'] = test.id.values\n",
    "submission = submission.rename(columns = {0:'target'})\n",
    "submission = submission[['id','target']]\n",
    "submission.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T07:14:53.958733Z",
     "iopub.status.busy": "2021-05-26T07:14:53.958339Z",
     "iopub.status.idle": "2021-05-26T07:14:53.970035Z",
     "shell.execute_reply": "2021-05-26T07:14:53.968756Z",
     "shell.execute_reply.started": "2021-05-26T07:14:53.958701Z"
    }
   },
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T07:14:54.374604Z",
     "iopub.status.busy": "2021-05-26T07:14:54.374222Z",
     "iopub.status.idle": "2021-05-26T07:14:54.38052Z",
     "shell.execute_reply": "2021-05-26T07:14:54.379561Z",
     "shell.execute_reply.started": "2021-05-26T07:14:54.374573Z"
    }
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T07:18:10.2576Z",
     "iopub.status.busy": "2021-05-26T07:18:10.257196Z",
     "iopub.status.idle": "2021-05-26T07:18:10.264272Z",
     "shell.execute_reply": "2021-05-26T07:18:10.263219Z",
     "shell.execute_reply.started": "2021-05-26T07:18:10.257565Z"
    }
   },
   "outputs": [],
   "source": [
    "test_data.excerpt.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
